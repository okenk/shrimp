---
title: "Shrimp modeling"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r, include = FALSE, message=FALSE, results='hide'}
library(rstan)
library(tidyverse)
library(here)
library(cowplot)
source(here('Code/length_data_manip.R'))

# extractor functions

plot_pro_devs <- function(list_of_draws, lengths, y.df) {
  yr.seq <- sort(unique(lengths$Year_Class))
  # Var2 has 30 unique values, is monthly time steps
  # Var3 has 32 unique values, is cohort
  pro_devs <- reshape2::melt(list_of_draws$pro_dev, 
                             value.name = 'pro_dev') %>% 
    mutate(Year_Class = yr.seq[Var3],
           Age_Month = round(as.numeric(
             names(y.df)[-(1:2)][Var2]),2),
           winter = Age_Month %in% c(1.83, 1.92, 2, 2.08, 2.17, 2.25,
                                     2.83, 2.92, 3, 3.08, 3.17, 3.25))
  
  for(cohort in yr.seq) {
    print(
      filter(pro_devs, Year_Class == cohort) %>%
        ggplot() +
        geom_violin(aes(x = factor(Age_Month), y = pro_dev, 
                        fill = winter)) +
        geom_hline(yintercept = 0, col = 'red') +
        theme_classic() +
        ggtitle(cohort) +
        xlab('Age (yrs)') +
        ylab('Process error (mm)') +
        scale_fill_manual(values = alpha(
          c('tomato', 'royalblue3'), alpha = 0.5)))
  }
}

calc_fitted_vals <- function(list_of_draws, y.mat, complete.data, 
                             y.df) {
  med.pred <- list_of_draws$pred_vec %>%
    apply(2, median)
  pred.mat <- matrix(nrow = nrow(y.mat), ncol = ncol(y.mat), 
                     dimnames = list(rownames(y.mat), colnames(y.mat)))
  for(ii in 1:nrow(complete.data)) {
    pred.mat[complete.data[ii,1], complete.data[ii,2]] <- med.pred[ii] 
  }
  
  length.fits <- as_tibble(pred.mat + mean(y.mat, na.rm=TRUE)) %>%
    bind_cols(select(y.df, Area, Year_Class)) %>%
    gather(key = 'Age_Month', value = 'MedPredLength', -Area, -Year_Class) %>%
    mutate(Age_Month = as.numeric(Age_Month),
           Age = floor(Age_Month),
           Month_Num = round(12*(Age_Month - Age))) %>% 
    select(-Age_Month) %>%
    right_join(lengths)
  
  return(length.fits)
}

# Plot posterior predictive distributions
plot_ppd_by_cohort <- function(length.fits, list_of_draws, offset) {
  rec.hist <- filter(length.fits, Age==1) %>% 
    group_by(Year_Class) %>% 
    summarize(Std_R = first(Std_R)) %>%
    ggplot() +
    geom_violin(aes(y = Std_R, x = 'R')) +
    theme_classic(8) +
    xlab('') +
    ylab('Rec')
  
  for(cohort in sort(unique(length.fits$Year_Class))) {
    this.cohort.data <- filter(length.fits, Year_Class == cohort)
    cohort.index <- which(length.fits$Year_Class == cohort)
    
    inset <- rec.hist 
    if(cohort >= 1988) {
      inset <- inset +
        annotate(geom = 'point', x = 'R',
                 y = this.cohort.data$Std_R[1], 
                 col = 'red')
    }
    
    main.plot <- bayesplot::ppc_violin_grouped(
      y = this.cohort.data$Avg_Len,
      yrep = list_of_draws$y_pp[,cohort.index] +
        mean(length.fits$Avg_Len) * offset, 
      group = factor(round(this.cohort.data$Age_Month, 2)),
      y_draw = 'points'
    ) +
      xlab('Age (yrs)') +
      ylab('Length (mm)') +
      ggtitle(cohort) +
      geom_vline(xintercept = c(7.5,14.5))
    
    print(
      ggdraw(main.plot) +
        draw_plot(inset, x = .15, y = .65,
                  width = .15, height = .3)
    )
  }
}  
# load(here('Code/model_fit.RData'))
# load(here('Code/model_fit_tables.RData'))
```

## Data:

Data we have at month-year-fishing area resolution:

-   Average length and sample size for each age class (1-3) from April to October, i.e., up to 21 observations of each year class in a given area, but some combinations are missing. This is from sampling the catch.
-   SD for each age class from 2014-2018 (actually have individual data for each shrimp measured in these years)
-   Total catch in lbs
-   Count per pound
-   Percent age composition of the catch by number
-   All of this for 1989-2018, though a lot of pubs seems to have 1980-present

Here is a (different) plot of the mean lengths at age by area and year class.

```{r, echo = FALSE, fig.height = 6, fig.width = 6, warning=FALSE, message=FALSE}
ggplot(lengths, aes(x = Age_Month, y = Avg_Len, group = paste(Year_Class, Area), col = Year_Class)) +
  geom_path(lwd = .25, alpha = 0.5) +
  geom_point(cex=.5, alpha = 0.5) +
  facet_wrap(~Area, drop = TRUE) +
  labs(x = 'Age (years)', y = 'Mean Length (mm)', color = 'Cohort') +
  NULL
```

Here is a map of the state areas:

![](../Figures/shrimp_areas.png){height="4in"}

For the "mega-region" analysis (see below), areas 12 and 18 form California, areas 19-22 are Southern Oregon, areas 24-28 are Northern Oregon, and areas 29 and higher are Washington. This division in Oregon matches the VPA.

### L vs $\Delta$ L

The following plot shows change in length vs length. It is a subset of the total data set and only includes cases where sampling was conducted in the same state area for two consecutive months. The red line is the linear regression. It is not great and very noisy.

```{r, echo=FALSE, message=FALSE}
lengths %>%
  group_by(Area, Year_Class) %>%
  arrange(Age_Month, .by_group = TRUE) %>%
  mutate(is_consecutive = round(c(diff(Age_Month),NA),2) == .08,
         delta_L = c(diff(Avg_Len), NA)) %>% 
  filter(is_consecutive) %>%
  ggplot(aes(x = Avg_Len, y = delta_L)) +
  geom_point(aes(col = Year_Class), alpha = 0.25) +
  stat_smooth(method = 'lm', col = 'red') +
  theme_classic() +
  xlab('Length (mm)') +
  ylab(expression(paste(Delta, 'Length (mm)')))

```

## Linear model:

$L_{a,y,r}$ is length of age $a$ (in months) shrimp from the cohort $c$ caught in area (region) r, and $m$ is the month of the year (i.e., cycles back to 1 at the end of the year unlike $a$). To align $m$ with the seasonal cycle of growth, $m=1$ corresponds with March.

$$\begin{aligned}
L_{1,c,r} &= \mu_{L1} + \gamma_{r} + \varepsilon_{1,c,r} \\
L_{a,c,r} &= \alpha_0 + \alpha_1 L_{a-1,c,r} + \alpha_s \sin\left(\tfrac{\pi m}{6}\right) + \varepsilon_{a,c} \\
\hat{L}_{a,c,r} &= L_{a,c,r} + \delta_{a,c,r}
\end{aligned}$$

$\mu_{L1}$ is average size of age 1 shrimp in April, $\alpha_0$ and $\alpha_1$ describe the average monthly growth, and $\alpha_s$ is the magnitude of the seasonality component.

Distributional assumptions are: $$\begin{aligned}
\varepsilon_{1,c,r} &\sim N(0, \sigma_{L1}),\varepsilon_{1,c,r_i}=\varepsilon_{1,c,r_j} \forall r_i,r_j\in R \\
\varepsilon_{a,c} &\sim N(0, \sigma_p), a\neq 1\\
\delta_{a,c,r} &\sim N(0, \sigma_o)\\
\gamma_r &\sim N(0, \sigma_r)
\end{aligned}$$

Note that the assumption for $\varepsilon_{1,c,r}$ implies that there is a single initial size estimated for each "mega-region" $R$ for each cohort $c$. There is a global mean initial size ($\mu_{L1}$), but there is no additional hierarchy (i.e., no mega-regions within years or years within mega-regions).

There was also a simpler model fit with the initial size equation as:

$$
L_{1,c,r} = \mu_{L1} + \gamma_{r} + \varepsilon_{1,c}
$$

The key change here is that the annual variation in initial size is coast-wide rather than estimated separately for each "mega-region." This would not permit an analysis at as detailed a level of organization as the recruitment, but was fit for comparison.

## Non-linear parameterization

A second parameterization was explored that allows the seasonal component to solely influence the growth rate, and not the asymptotic size. Only the second equation is different than the model above (instead of $\alpha_0$ and $\alpha_1$, the two model parameters are $L_\infty$ and $k$):

$$
L_{a,c,r} = L_\infty\left[1-\exp\left(-ke^{\alpha_s\sin\left(\pi m/6\right)} \right) \right] + L_{a-1,c,r}\exp\left(-ke^{\alpha_s\sin\left(\pi m/6\right)}\right) + \varepsilon_{a,c}
$$

Because earlier analyses showed similar results for this model but much longer run times, results from it are not included.

## Fitting:
```{r, echo=FALSE}
load(here('Code', 'model_fit_big_area.RData'))
mod.summary <- summary(mod)
```


The model is fit in Stan as a multivariate autoregressive state-space model. For the linear model, the global mean is subtracted from all lengths to improve numerical stability. Each year class is essentially a different "state", and each area represents a different observation process of the same state. Four chains are run for 4000 iterations, and the final 2000 iterations of each chain are retained. The lowest effective sample size is `r round(min(mod.summary$summary[,'n_eff']), 0)` and the largest $\hat{R}$ is `r round(max(mod.summary$summary[,'Rhat']), 3)`, indicating chains have converged and produced a sufficient number of independent draws to conduct inference. It takes `r round(max(apply(get_elapsed_time(mod), 1, sum))/60, 1)` minutes to run on my laptop including warmup, with the chains running in parallel. We may be able to decrease iterations a bit more for efficiency

# Results:

First, here is a table of the estimated parameters.

```{r, echo = FALSE}
mod.summary.subset <- mod.summary$summary[c('x0_mean', 'sigma_x0', 'sigma_area', 'U_season', 'U', 'B', 'sigma_process', 'sigma_obs'),]
row.names(mod.summary.subset) <- c('$\\mu_{L1}$', '$\\sigma_{L1}$', '$\\sigma_r$', '$\\alpha_s$', '$\\alpha_0$', '$\\alpha_1$', '$\\sigma_p$', '$\\sigma_o$')
knitr::kable(mod.summary.subset, digits = 2)
```

Second, here is a plot of residuals versus fitted values (using the median from the posterior as the fitted value) with a smoother. We see that the residuals look alright.

```{r, echo = FALSE, warning=FALSE, message=FALSE}
list_of_draws <- rstan::extract(mod)

length.fits <- calc_fitted_vals(list_of_draws = list_of_draws, y.mat = y.mat, 
                                complete.data = complete.data, y.df = y.df)

ggplot(length.fits, aes(x = MedPredLength, y = Avg_Len - MedPredLength)) +
  geom_point(alpha = .5) + 
  stat_smooth() +
  geom_hline(yintercept = 0, col = 'red') +
  theme(legend.position = 'none') +
  labs(x = 'Median fitted value', y = 'Median residual') +
  NULL  
```

```{r, echo = FALSE, warning=FALSE}
ggplot(length.fits) +
  geom_boxplot(aes(x = factor(Year), y = Avg_Len - MedPredLength)) +
  geom_hline(yintercept = 0, col = 'red') +
  labs(y = 'Median residual', title = 'Residuals versus year', x = 'Year') +
  scale_x_discrete(labels = c('', 1990, rep('', 9), 2000, rep('', 9), 2010, rep('', 8)))

ggplot(length.fits) +
  geom_boxplot(aes(x = factor(Month_Num), y = Avg_Len - MedPredLength)) +
  geom_hline(yintercept = 0, col = 'red') +
  labs(x = 'Month', y = 'Median residual', title = 'Residuals versus Month')

```

I also thought it would be useful to look at the process errors. We can see that the process model under-estimates growth for age 1 and over-estimates it for age 3. Oddly, $\alpha_1$ is being estimated very close to its bound at 1. If it were smaller, that would allow growth to slow as shrimp get larger. Strong priors with minimal weight near 1 did not change this, but did slow mixing.

```{r, echo = FALSE}
yr.seq <- sort(unique(lengths$Year_Class))
# Var2 has 30 unique values, is monthly time steps
# Var3 has 32 unique values, is cohort
pro_devs <- reshape2::melt(list_of_draws$pro_dev, 
                           value.name = 'pro_dev') %>% 
  mutate(Year_Class = yr.seq[Var3],
         Age_Month = round(as.numeric(
           grep('[0-9]', names(y.df), value=TRUE)[Var2]),2),
         winter = Age_Month %in% c(1.83, 1.92, 2, 2.08, 2.17, 2.25,
                                   2.83, 2.92, 3, 3.08, 3.17, 3.25))

ggplot(pro_devs) + 
  geom_boxplot(aes(x = factor(Age_Month), y = pro_dev, col = winter)) +
  geom_hline(yintercept = 0, col = 'red') +
  scale_x_discrete(labels = c(1, rep('', 11), 2, rep('', 11), 3, rep('', 5))) +
  labs(x = 'Age (years)', y = 'Process error')

ggplot(pro_devs) + 
  geom_boxplot(aes(x = factor(Year_Class), y = pro_dev)) +
  geom_hline(yintercept = 0, col = 'red') +
  scale_x_discrete(labels = c(rep('', 4), 1990, rep('', 9), 2000, rep('', 9), 2010, rep('', 7))) +
  labs(x = 'Cohort', y = 'Process error') +
  theme(legend.position = 'none')

```

## Next steps:

1.  Model selection?

<!-- Martell et al. 2000 report annual M of 0.96 $\text{yr}^{-1}$, which is used in VPA.  -->
